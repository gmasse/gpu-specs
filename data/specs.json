{
    "_header": {
        "name": { "full_name": "GPU Name" },
        "fp64": { "full_name": "FP64", "unit": "TFLOPS" },
        "fp64_tensor_core": { "full_name": "FP64 Tensor Core", "unit": "TFLOPS" },
        "fp32": { "full_name": "FP32", "unit": "TFLOPS" },
        "tf32_tensor_core": { "full_name": "TF32 Tensor Core", "unit": "TFLOPS" },
        "tf32_tensor_core_sparsity": { "full_name": "TF32 Tensor Core with Sparsity", "unit": "TFLOPS" },
        "fp16": { "full_name": "FP16", "unit": "TFLOPS" },
        "fp16_tensor_core": { "full_name": "FP16 Tensor Core", "unit": "TFLOPS" },
        "fp16_tensor_core_sparsity": { "full_name": "FP16 Tensor Core with Sparsity", "unit": "TFLOPS" },
        "bf16_tensor_core": { "full_name": "BF16 Tensor Core", "unit": "TFLOPS" },
        "bf16_tensor_core_sparsity": { "full_name": "BF16 Tensor Core with Sparsity", "unit": "TFLOPS" },
        "fp8_tensor_core": { "full_name": "FP8 Tensor Core", "unit": "TFLOPS" },
        "fp8_tensor_core_sparsity": { "full_name": "FP8 Tensor Core with Sparsity", "unit": "TFLOPS" },
        "fp4_tensor_core": { "full_name": "FP4 Tensor Core", "unit": "TFLOPS" },
        "fp4_tensor_core_sparsity": { "full_name": "FP4 Tensor Core with Sparsity", "unit": "TFLOPS" },
        "int8_tensor_core": { "full_name": "INT8 Tensor Core", "unit": "TOPS" },
        "int8_tensor_core_sparsity": { "full_name": "INT8 Tensor Core with Sparsity", "unit": "TOPS" },
        "int4_tensor_core": { "full_name": "INT4 Tensor Core", "unit": "TOPS" },
        "int4_tensor_core_sparsity": { "full_name": "INT4 Tensor Core with Sparsity", "unit": "TOPS" },
        "manufacturer": { "full_name": "Manufacturer" },
        "architecture": { "full_name": "Architecture" },
        "process": { "full_name": "Manufacturing Process" },
        "nvidia_rt_cores": { "full_name": "NVIDIA RT Cores"},
        "nvidia_rt_cores_generation": { "full_name": "NVIDIA RT Cores Generation"},
        "nvidia_tensor_cores": { "full_name": "NVIDIA Tensor Cores" },
        "nvidia_tensor_cores_generation": { "full_name": "NVIDIA Tensor Cores Generation" },
        "nvidia_cuda_cores": { "full_name": "NVIDIA CUDA Cores" },
        "gpu_memory": { "full_name": "GPU Memory", "unit": "GB" },
        "memory_type": { "full_name": "Memory Type" },
        "memory_bandwidth": { "full_name": "Memory Bandwidth", "unit": "GB/s" },
        "interconnect": { "full_name": "Interconnect Type" },
        "encoders_decoders": { "full_name": "Encoders and Decoders" },
        "cuda_compute_capability": { "full_name": "CUDA Compute Capability" },
        "power_consumption": { "full_name": "Power Consumption", "unit": "W" },
        "die_size": { "full_name": "Die Size", "unit": "mm2" }
    },
    "h100": {
        "name": "H100",
        "fp64": 25.6,
        "fp64_tensor_core": 51,
        "fp32": 51.2,
        "tf32_tensor_core": null,
        "tf32_tensor_core_sparsity": 756,
        "fp16": 204.9,
        "fp16_tensor_core": null,
        "fp16_tensor_core_sparsity": null,
        "bf16_tensor_core": null,
        "bf16_tensor_core_sparsity": 1513,
        "fp8_tensor_core": null,
        "fp8_tensor_core_sparsity": 3026,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": null,
        "int8_tensor_core_sparsity": 3026,
        "int4_tensor_core": null,
        "int4_tensor_core_sparsity": null,
        "manufacturer": "NVIDIA",
        "architecture": "Hopper",
        "process": null,
        "nvidia_rt_cores": null,
        "nvidia_rt_cores_generation": null,
        "nvidia_tensor_cores": 456,
        "nvidia_tensor_cores_generation": 4,
        "nvidia_cuda_cores": 14592,
        "gpu_memory": 80,
        "memory_type": "HBM2e",
        "memory_bandwidth": 2048,
        "interconnect": "PCIe Gen5",
        "encoders_decoders": "0, 7",
        "cuda_compute_capability": "9",
        "power_consumption": 350,
        "die_size": null,
        "sources": [
            "https://getdeploying.com/reference/cloud-gpu/nvidia-h100",
            "https://www.techpowerup.com/gpu-specs/h100-pcie-80-gb.c3899",
            "https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new"
        ]
    },
    "l40s": {
        "name": "L40S",
        "fp64": 1.4,
        "fp64_tensor_core": 1.4,
        "fp32": 91.6,
        "tf32_tensor_core": 183,
        "tf32_tensor_core_sparsity": 366,
        "fp16": 91.6,
        "fp16_tensor_core": 362,
        "fp16_tensor_core_sparsity": 733,
        "bf16_tensor_core": 362,
        "bf16_tensor_core_sparsity": 733,
        "fp8_tensor_core": 733,
        "fp8_tensor_core_sparsity": 1466,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 733,
        "int8_tensor_core_sparsity": 1466,
        "int4_tensor_core": 733,
        "int4_tensor_core_sparsity": 1466,
        "manufacturer": "NVIDIA",
        "architecture": "Ada Lovelace",
        "process": null,
        "nvidia_rt_cores": 142,
        "nvidia_rt_cores_generation": 3,
        "nvidia_tensor_cores": 568,
        "nvidia_tensor_cores_generation": 4,
        "nvidia_cuda_cores": 18176,
        "gpu_memory": 48,
        "memory_type": "GDDR6",
        "memory_bandwidth": 864,
        "interconnect": "PCIe Gen4",
        "encoders_decoders": "3, 3",
        "cuda_compute_capability": "8.9",
        "power_consumption": 300,
        "die_size": null,
        "sources": [
            "https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413",
            "https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf"
        ]
    },
    "l4": {
        "name": "L4",
        "fp64": 0.5,
        "fp64_tensor_core": 0.5,
        "fp32": 30.3,
        "tf32_tensor_core": 60,
        "tf32_tensor_core_sparsity": 120,
        "fp16": 30.3,
        "fp16_tensor_core": 121,
        "fp16_tensor_core_sparsity": 242,
        "bf16_tensor_core": 121,
        "bf16_tensor_core_sparsity": 242,
        "fp8_tensor_core": 242,
        "fp8_tensor_core_sparsity": 485,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 242,
        "int8_tensor_core_sparsity": 485,
        "int4_tensor_core": null,
        "int4_tensor_core_sparsity": null,
        "manufacturer": "NVIDIA",
        "architecture": "Ada Lovelace",
        "process": null,
        "nvidia_rt_cores": 60,
        "nvidia_rt_cores_generation": 3,
        "nvidia_tensor_cores": 240,
        "nvidia_tensor_cores_generation": 4,
        "nvidia_cuda_cores": 7424,
        "gpu_memory": 24,
        "memory_type": "GDDR6",
        "memory_bandwidth": 300,
        "interconnect": "PCIe Gen4",
        "encoders_decoders": "2, 4",
        "cuda_compute_capability": "8.9",
        "power_consumption": 72,
        "die_size": null,
        "sources": [
            "https://resources.nvidia.com/en-us-data-center-overview/l4-gpu-datasheet",
            "https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf"
        ]
    },
    "rtx4090": {
        "name": "GeForce RTX 4090",
        "fp64": 1.3,
        "fp64_tensor_core": 1.3,
        "fp32": 82.6,
        "tf32_tensor_core": 82.6,
        "tf32_tensor_core_sparsity": 165.2,
        "fp16": 82.6,
        "fp16_tensor_core": 330.3,
        "fp16_tensor_core_sparsity": 660.6,
        "bf16_tensor_core": 165.2,
        "bf16_tensor_core_sparsity": 330.4,
        "fp8_tensor_core": 660.6,
        "fp8_tensor_core_sparsity": 1321.2,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 660.6,
        "int8_tensor_core_sparsity": 1321.2,
        "int4_tensor_core": 1321.2,
        "int4_tensor_core_sparsity": 2642.4,
        "manufacturer": "NVIDIA",
        "architecture": "Ada Lovelace",
        "process": "TSMC 4N",
        "nvidia_rt_cores": 128,
        "nvidia_rt_cores_generation": 3,
        "nvidia_tensor_cores": 512,
        "nvidia_tensor_cores_generation": 4,
        "nvidia_cuda_cores": 16384,
        "gpu_memory": 24,
        "memory_type": "GDDR6X",
        "memory_bandwidth": 1008,
        "interconnect": "PCIe Gen4",
        "encoders_decoders": "2, 1",
        "cuda_compute_capability": "8.9",
        "power_consumption": 450,
        "die_size": 608.5,
        "sources": [
            "https://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889",
            "https://images.nvidia.com/aem-dam/Solutions/geforce/ada/nvidia-ada-gpu-architecture.pdf",
            "https://developer.nvidia.com/cuda-gpus"
        ]
    },
    "a100_pcie_40gb": {
        "name": "A100 PCIe 40GB",
        "fp64": 9.7,
        "fp64_tensor_core": 19.5,
        "fp32": 19.5,
        "tf32_tensor_core": 156,
        "tf32_tensor_core_sparsity": 312,
        "fp16": 78,
        "fp16_tensor_core": 312,
        "fp16_tensor_core_sparsity": 624,
        "bf16_tensor_core": 312,
        "bf16_tensor_core_sparsity": 624,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 624,
        "int8_tensor_core_sparsity": 1248,
        "int4_tensor_core": null,
        "int4_tensor_core_sparsity": null,
        "manufacturer": "NVIDIA",
        "architecture": "Ampere",
        "process": null,
        "nvidia_rt_cores": null,
        "nvidia_rt_cores_generation": null,
        "nvidia_tensor_cores": 432,
        "nvidia_tensor_cores_generation": 3,
        "nvidia_cuda_cores": 6912,
        "gpu_memory": 40,
        "memory_type": "HBM2e",
        "memory_bandwidth": 1555,
        "interconnect": "PCIe Gen4",
        "encoders_decoders": "0, 5",
        "cuda_compute_capability": "8.0",
        "power_consumption": 250,
        "die_size": null,
        "sources": [
            "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf"
        ]
    },
    "a100_pcie_80gb": {
        "name": "A100 PCIe 80GB",
        "fp64": 9.7,
        "fp64_tensor_core": 19.5,
        "fp32": 19.5,
        "tf32_tensor_core": 156,
        "tf32_tensor_core_sparsity": 312,
        "fp16": 78,
        "fp16_tensor_core": 312,
        "fp16_tensor_core_sparsity": 624,
        "bf16_tensor_core": 312,
        "bf16_tensor_core_sparsity": 624,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 624,
        "int8_tensor_core_sparsity": 1248,
        "int4_tensor_core": null,
        "int4_tensor_core_sparsity": null,
        "manufacturer": "NVIDIA",
        "architecture": "Ampere",
        "process": null,
        "nvidia_rt_cores": null,
        "nvidia_rt_cores_generation": null,
        "nvidia_tensor_cores": 432,
        "nvidia_tensor_cores_generation": 3,
        "nvidia_cuda_cores": 6912,
        "gpu_memory": 80,
        "memory_type": "HBM2e",
        "memory_bandwidth": 1935,
        "interconnect": "PCIe Gen4",
        "encoders_decoders": "0, 5",
        "cuda_compute_capability": "8",
        "power_consumption": 300,
        "die_size": null,
        "sources": [
            "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf",
            "https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new"
        ]
    },
    "a100_sxm4_40gb": {
        "name": "A100 SXM4 40GB",
        "fp64": 9.7,
        "fp64_tensor_core": 19.5,
        "fp32": 19.5,
        "tf32_tensor_core": 156,
        "tf32_tensor_core_sparsity": 312,
        "fp16": 78,
        "fp16_tensor_core": 312,
        "fp16_tensor_core_sparsity": 624,
        "bf16_tensor_core": 312,
        "bf16_tensor_core_sparsity": 624,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 624,
        "int8_tensor_core_sparsity": 1248,
        "int4_tensor_core": null,
        "int4_tensor_core_sparsity": null,
        "manufacturer": "NVIDIA",
        "architecture": "Ampere",
        "process": null,
        "nvidia_rt_cores": null,
        "nvidia_rt_cores_generation": null,
        "nvidia_tensor_cores": 432,
        "nvidia_tensor_cores_generation": 3,
        "nvidia_cuda_cores": 6912,
        "gpu_memory": 40,
        "memory_type": "HBM2e",
        "memory_bandwidth": 1555,
        "interconnect": "NVLink",
        "encoders_decoders": "0, 5",
        "cuda_compute_capability": "8.0",
        "power_consumption": 400,
        "die_size": null,
        "sources": [
            "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf"
        ]
    },
    "a100_sxm4_80gb": {
        "name": "A100 SXM4 80GB",
        "fp64": 9.7,
        "fp64_tensor_core": 19.5,
        "fp32": 19.5,
        "tf32_tensor_core": 156,
        "tf32_tensor_core_sparsity": 312,
        "fp16": 78,
        "fp16_tensor_core": 312,
        "fp16_tensor_core_sparsity": 624,
        "bf16_tensor_core": 312,
        "bf16_tensor_core_sparsity": 624,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 624,
        "int8_tensor_core_sparsity": 1248,
        "int4_tensor_core": null,
        "int4_tensor_core_sparsity": null,
        "manufacturer": "NVIDIA",
        "architecture": "Ampere",
        "process": null,
        "nvidia_rt_cores": null,
        "nvidia_rt_cores_generation": null,
        "nvidia_tensor_cores": 432,
        "nvidia_tensor_cores_generation": 3,
        "nvidia_cuda_cores": 6912,
        "gpu_memory": 80,
        "memory_type": "HBM2e",
        "memory_bandwidth": 2039,
        "interconnect": "NVLink",
        "encoders_decoders": "0, 5",
        "cuda_compute_capability": "8",
        "power_consumption": 400,
        "die_size": null,
        "sources": [
            "https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf",
            "https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new"
        ]
    },
    "a10": {
        "name": "A10",
        "fp64": 0.5,
        "fp64_tensor_core": 0.5,
        "fp32": 31.2,
        "tf32_tensor_core": 62.5,
        "tf32_tensor_core_sparsity": 125,
        "fp16": 31.2,
        "fp16_tensor_core": null,
        "fp16_tensor_core_sparsity": null,
        "bf16_tensor_core": 125,
        "bf16_tensor_core_sparsity": 250,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 250,
        "int8_tensor_core_sparsity": 500,
        "int4_tensor_core": 500,
        "int4_tensor_core_sparsity": 1000,
        "manufacturer": "NVIDIA",
        "architecture": "Ampere",
        "process": null,
        "nvidia_rt_cores": 72,
        "nvidia_rt_cores_generation": 2,
        "nvidia_tensor_cores": 288,
        "nvidia_tensor_cores_generation": 3,
        "nvidia_cuda_cores": 9216,
        "gpu_memory": 24,
        "memory_type": "GDDR6",
        "memory_bandwidth": 600,
        "interconnect": "PCIe Gen4",
        "encoders_decoders": "1, 2",
        "cuda_compute_capability": "8.6",
        "power_consumption": 150,
        "die_size": null,
        "sources": [
            "https://resources.nvidia.com/en-us-gpu/a10-datasheet-nvidia",
            "https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.1.pdf",
            "https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new"
        ]
    },
    "t4": {
        "name": "T4",
        "fp64": 0.2,
        "fp64_tensor_core": 0,
        "fp32": 8.1,
        "tf32_tensor_core": 0,
        "tf32_tensor_core_sparsity": 0,
        "fp16": 65,
        "fp16_tensor_core": null,
        "fp16_tensor_core_sparsity": null,
        "bf16_tensor_core": 0,
        "bf16_tensor_core_sparsity": 0,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 130,
        "int8_tensor_core_sparsity": null,
        "int4_tensor_core": 260,
        "int4_tensor_core_sparsity": null,
        "manufacturer": "NVIDIA",
        "architecture": "Turing",
        "process": null,
        "nvidia_rt_cores": null,
        "nvidia_rt_cores_generation": null,
        "nvidia_tensor_cores": 320,
        "nvidia_tensor_cores_generation": 2,
        "nvidia_cuda_cores": 2560,
        "gpu_memory": 16,
        "memory_type": "GDDR6",
        "memory_bandwidth": 300,
        "interconnect": "PCIe Gen3",
        "encoders_decoders": "1, 2",
        "cuda_compute_capability": "7.5",
        "power_consumption": 70,
        "die_size": null,
        "sources": [
            "https://getdeploying.com/reference/cloud-gpu/nvidia-t4",
            "https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf",
            "https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new",
            "https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/solutions/resources/documents1/Datasheet_NVIDIA_T4_Virtualization.pdf"
        ]
    },
    "quadro_rtx_5000": {
        "name": "Quadro RTX 5000",
        "fp64": 0.3,
        "fp64_tensor_core": 0,
        "fp32": 11.2,
        "tf32_tensor_core": 0,
        "tf32_tensor_core_sparsity": 0,
        "fp16": 22.3,
        "fp16_tensor_core": null,
        "fp16_tensor_core_sparsity": null,
        "bf16_tensor_core": 0,
        "bf16_tensor_core_sparsity": 0,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": null,
        "int8_tensor_core_sparsity": null,
        "int4_tensor_core": null,
        "int4_tensor_core_sparsity": null,
        "manufacturer": "NVIDIA",
        "architecture": "Turing",
        "process": null,
        "nvidia_rt_cores": 48,
        "nvidia_rt_cores_generation": null,
        "nvidia_tensor_cores": 384,
        "nvidia_tensor_cores_generation": 2,
        "nvidia_cuda_cores": 3072,
        "gpu_memory": 16,
        "memory_type": "GDDR6",
        "memory_bandwidth": 448,
        "interconnect": "PCIe Gen3",
        "encoders_decoders": "1, 2",
        "cuda_compute_capability": "7.5",
        "power_consumption": 230,
        "die_size": null,
        "sources": [
            "https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/quadro-product-literature/quadro-rtx-5000-data-sheet-us-nvidia-704120-r4-web.pdf",
            "https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf"

        ]
    },
    "v100_pcie": {
        "name": "V100 PCIe",
        "fp64": 7.1,
        "fp64_tensor_core": 0,
        "fp32": 14.1,
        "tf32_tensor_core": 112,
        "tf32_tensor_core_sparsity": 0,
        "fp16": 28.3,
        "fp16_tensor_core": 112,
        "fp16_tensor_core_sparsity": null,
        "bf16_tensor_core": 0,
        "bf16_tensor_core_sparsity": 0,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 0,
        "int8_tensor_core_sparsity": 0,
        "int4_tensor_core": 0,
        "int4_tensor_core_sparsity": 0,
        "manufacturer": "NVIDIA",
        "architecture": "Volta",
        "process": null,
        "nvidia_rt_cores": 0,
        "nvidia_tensor_cores": 640,
        "nvidia_tensor_cores_generation": 1,
        "nvidia_cuda_cores": 5120,
        "gpu_memory": "16/32",
        "memory_type": "HBM2",
        "memory_bandwidth": 900,
        "interconnect": "PCIe Gen3",
        "encoders_decoders": "3, 1",
        "cuda_compute_capability": "7",
        "power_consumption": 250,
        "die_size": null,
        "sources": [
            "https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf",
            "https://www.techpowerup.com/gpu-specs/tesla-v100-pcie-32-gb.c3184",
            "https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new"
        ]
    },
    "v100_sxm2": {
        "name": "V100 SXM2",
        "fp64": 7.8,
        "fp64_tensor_core": 0,
        "fp32": 15.7,
        "tf32_tensor_core": 125,
        "tf32_tensor_core_sparsity": 0,
        "fp16": 31.3,
        "fp16_tensor_core": 125,
        "fp16_tensor_core_sparsity": null,
        "bf16_tensor_core": 0,
        "bf16_tensor_core_sparsity": 0,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 0,
        "int8_tensor_core_sparsity": 0,
        "int4_tensor_core": 0,
        "int4_tensor_core_sparsity": 0,
        "manufacturer": "NVIDIA",
        "architecture": "Volta",
        "process": null,
        "nvidia_rt_cores": 0,
        "nvidia_tensor_cores": 640,
        "nvidia_tensor_cores_generation": 1,
        "nvidia_cuda_cores": 5120,
        "gpu_memory": "16/32",
        "memory_type": "HBM2",
        "memory_bandwidth": 900,
        "interconnect": "NVLink",
        "encoders_decoders": "3, 1",
        "cuda_compute_capability": "7",
        "power_consumption": 300,
        "die_size": null,
        "sources": [
            "https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf",
            "https://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-16-gb.c3018",
            "https://www.techpowerup.com/gpu-specs/tesla-v100-sxm2-32-gb.c3183",
            "https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new"
        ]
    },
    "v100s_pcie": {
        "name": "V100S PCIe",
        "fp64": 8.2,
        "fp64_tensor_core": 0,
        "fp32": 16.4,
        "tf32_tensor_core": 0,
        "tf32_tensor_core_sparsity": 0,
        "fp16": 32.8,
        "fp16_tensor_core": 130,
        "fp16_tensor_core_sparsity": null,
        "bf16_tensor_core": 0,
        "bf16_tensor_core_sparsity": 0,
        "fp8_tensor_core": 0,
        "fp8_tensor_core_sparsity": 0,
        "fp4_tensor_core": 0,
        "fp4_tensor_core_sparsity": 0,
        "int8_tensor_core": 0,
        "int8_tensor_core_sparsity": 0,
        "int4_tensor_core": 0,
        "int4_tensor_core_sparsity": 0,
        "manufacturer": "NVIDIA",
        "architecture": "Volta",
        "process": null,
        "nvidia_rt_cores": 0,
        "nvidia_tensor_cores": 640,
        "nvidia_tensor_cores_generation": 1,
        "nvidia_cuda_cores": 5120,
        "gpu_memory": 32,
        "memory_type": "HBM2",
        "memory_bandwidth": 1134,
        "interconnect": "PCIe Gen3",
        "encoders_decoders": "3, 1",
        "cuda_compute_capability": "7",
        "power_consumption": 250,
        "die_size": null,
        "sources": [
            "https://images.nvidia.com/content/technologies/volta/pdf/volta-v100-datasheet-update-us-1165301-r5.pdf",
            "https://www.techpowerup.com/gpu-specs/tesla-v100s-pcie-32-gb.c3584",
            "https://developer.nvidia.com/video-encode-and-decode-gpu-support-matrix-new"
        ]
    }
}